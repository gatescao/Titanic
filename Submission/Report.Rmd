---
title: "Surviving the Titanic"
author: "Gates Cao & Robert Wan"
date: "March 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
```

# Background
This analysis aims to predict survival of a given passenger of the Titanic. We obtained the dataset from Kaggle and used over 800 observations to perform modelling and cross-validation. Methods used include logistic regression, decision tree, bagging, random forest, boosting, KNN, LDA, and QDA. The decision tree method turned out to be the best because it provided a 0.799 testing accuracy. The testing accuracy was calculated by cross-validating with a separate test dataset, in which the column that represents survival is withheld by Kaggle.

# Dataset Description
We obtained the datasets from an ongoing competition on Kaggle. Kaggle provides two sets of data, a train dataset of 891 observations and a test dataset of 418 observations. Data on survival has been removed for the test dataset. Competitors use the train dataset to train and test their models and Kaggle uses the test dataset for ranking.

Aside from the difference on survival data, the train and test datsets have the same 11 other variables: `PassengerId`, `Pclass`, `Name`, `Sex`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Cabin`, `Embarked`. A definition of the variable names can be found on competition webpage at: https://www.kaggle.com/c/titanic/data.
```{r, include=FALSE}
#Load Packages
library(tidyverse)
library(modelr)
library(readr)
library(reshape2)
library(leaps)

# Read in data
train <- read_csv(file = "data/train.csv") %>% as_tibble()
test <- read_csv(file = "data/test.csv") %>% as_tibble()
```

# Data Preparation
### Missing Data
First we dropped four variables: `PassengerId`, `Name`, `Ticket`, and `Cabin` because they should not affect survival through any means.
```{r, include=FALSE}
# Drop unwanted variables
train <- train %>% dplyr::select(-PassengerId, -Name, -Ticket, -Cabin)
```

We then went on to deal with missing variables. Roughly 20% of the observations do not have data on `Age` and 2 observations do not have data on `Embarked`.
```{r, echo=FALSE}
colSums(is.na(train))
```

We replaced all missing `Age` data with the median age and missing `Embarked` data with "S" (Southampton), which is where most passengers embarked.
```{r, include=FALSE}
train$Age[is.na(train$Age)] <- median(na.omit(train$Age))
train$Embarked[is.na(train$Embarked)] <- "S"
```

### Splitting Data for Cross Validation
The test dataset does not have data on survival, so we had to use some of the observations in the train dataset to cross-validate our models before submitting to Kaggle. We randomly selected 80% of the train dataset for training and used the remaining 20% for cross-validation.
```{r, echo=FALSE}
set.seed(1)
training <- train %>% sample_frac(0.8)
validation <- setdiff(train, training)

training
validation
```

# Exploratory Data Analysis
To get some insights on the data before modelling, we first did some EDA.

### Age and Survival
```{r, echo=FALSE}
train %>%
  mutate(Age = cut(train$Age, 8)) %>%
  group_by(Age) %>%
  summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(Age, survival_rate)) +
  geom_col(aes(fill = Age)) +
  theme(axis.text.x  =element_text(angle = 45, hjust = 1), legend.position = "none")
```

At a survival rate of 60%, children under the age 10 are the age group that are most likely to survive. People in their thirties are also likely to survive, with a survival rate just below 50%. On the other hand, those above 60 and between 20 and 30 are the least likely to survive. We believe the lower survival rates of people between 10 and 30 is cause by the fact that a large number of passengers in the third class are in these two age groups. As we will see shortly, passengers in the third class have very lower survival rates.

### Cabin Class and Survival
```{r, echo=FALSE}
train %>% group_by(Pclass) %>% summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(Pclass, survival_rate)) +
  geom_col(aes(fill = Pclass)) +
  theme(legend.position = "none")
```

Over 60% of the passengers from the first class survived while less than 30% of the passengers from the third class did. This result may be caused by two factors. Passengers from the first class were probably given priority to board the life rafts. Third-class cabins were also near the bottom of the ship, which made escaping to the deck much more difficult.

### Sex and survival
```{r, echo=FALSE}
train %>% group_by(Sex) %>% summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(Sex, survival_rate)) +
  geom_col(aes(fill = Sex)) +
  theme(legend.position = "none")
```

Nearly 75% of female passengers survived but less than 20% of male passengers did. This was not due to more females being first class passengers or more females being girls younger than 10 years old. Female passengers were indeed given priorities to board the life rafts.

```{r, echo=FALSE}
train %>%
  mutate(Age = cut(train$Age, 8)) %>%
  group_by(Pclass, Sex) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))

train %>%
  mutate(Age = cut(train$Age, 8)) %>%
  group_by(Age, Sex) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  filter(Age == "(0.34,10.4]")
```

### Port of Embarkment and Survival
```{r, echo=FALSE}
train %>% group_by(Embarked) %>% summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(Embarked, survival_rate)) +
  geom_col(aes(fill = Embarked)) +
  xlab("Port of Embarkment") +
  scale_x_discrete(labels=c("C" = "Cherbourg", "Q" = "Queenstown", "S" = "Southampton")) +
  theme(legend.position = "none")
```

Interestingly, those who boarded at Cherbourg had a survival rate of 55%, over 60% higher than those who boarded at Southampton. This is perhaps due to the fact that over 50% of the Cherbourg passengers went to the first class. However, those 90% of Queeston passengers belonged to the third class but passengers who boarded at Queenstown still had a higher survival rate than Southampton. This may be due to random chance.

```{r,echo=FALSE}
train %>%
  group_by(Embarked, Pclass) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n))
```

### No. of Siblings and Spouse on Board and Survival
```{r, echo=FALSE}
train %>% group_by(SibSp) %>% summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(SibSp, survival_rate)) +
  geom_col(aes(fill = SibSp)) +
  xlab("No. of Siblings and Spouse") +
  theme(legend.position = "none")
```

Those with 1 or 2 spouse or siblings on board had the highest survival rate. The lower survival rate of those without any spouse or siblings on board was due to the fact that many of these passengering were from the third class.

```{r,echo=FALSE}
train %>%
  group_by(SibSp, Pclass) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  filter(SibSp == 0)
```

### No. of Parents and Children on Board and Survival
```{r, echo=FALSE}
train %>% group_by(Parch) %>% summarize(survival_rate = mean(Survived)) %>%
  ggplot(aes(Parch, survival_rate)) +
  geom_col(aes(fill = Parch)) +
  xlab("No. of Parents and Children on Board") +
  theme(legend.position = "none")
```

Those with 1 to 3 parents or children on board are more likely to survive than those without any parents or children on board. One thing to note here is that there are 15 observations in total for those with 3 or more parents or children on board. Therefore, insights about those with 3 or more parents or children on board are likely inaccurate.

```{r,echo=FALSE}
train %>%
  group_by(Parch, Sex) %>%
  summarize(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  filter(Parch > 3)
```

# Feature Generation
```{r,include=FALSE}
training <- training %>%
  mutate(Sex = ifelse(Sex == "male", 1, 0),
         Embarked = sapply(Embarked, switch, "C" = 0, "Q" = 1, "S" = 2))

validation <- validation %>%
  mutate(Sex = ifelse(Sex == "male", 1, 0),
         Embarked = sapply(Embarked, switch, "C" = 0, "Q" = 1, "S" = 2))

test <- test%>%
  mutate(Sex = ifelse(Sex == "male", 1, 0),
         Embarked = sapply(Embarked, switch, "C" = 0, "Q" = 1, "S" = 2))
```
```{r, echo=FALSE}
# Best Subset Selection
best_fits <- regsubsets(Survived ~ ., data = training, nvmax = 10)
best_fits_results <- summary(best_fits)

best_fit_data <- tibble(
  num_pred = 1:7,
  RSS = best_fits_results$rss,
  R2 = best_fits_results$rsq,
  Adj_R2 = best_fits_results$adjr2,
  Cp = best_fits_results$cp,
  BIC = best_fits_results$bic
) %>%
  gather(key = "statistic", value= "value", - num_pred)


# Plot the graph for each RSS, Adj_R2, Cp, and BIC
best_fit_data %>%
  filter(statistic != "R2") %>%
  ggplot(aes(x = num_pred, y = value)) +
  geom_point() +
  geom_line() +
  ggtitle("Best Subset Selection")
  facet_wrap(~ statistic, ncol = 2, scales = "free")

# Forward stepwise selection
fwd_fit <- regsubsets(Survived ~ ., 
                      data = training, 
                      nvmax = 7, 
                      method = "forward")

# Backward stepwise selection
bwd_fit <- regsubsets(Survived ~ ., 
                      data = training, 
                      nvmax = 7, 
                      method = "backward")

# Get the results
fwd_results <- summary(fwd_fit)
bwd_results <- summary(bwd_fit)

# Define a helper function
regsub_plot <- function(df_regsubset)
{ # get results data
  df <- summary(df_regsubset)
  
  # reshape data
  best_fit_data <- tibble(
    num_pred = 1:length(df$rss),
    RSS = df$rss,
    Rsqr = df$rsq,
    Adjr2 = df$adjr2,
    Cp = df$cp,
    BIC = df$bic
  ) %>%
    gather(key = "statistic", value = "value", -num_pred)
  
  # make the plot
  best_fit_data %>%
    filter(statistic != "R2") %>%
    ggplot(aes(x = num_pred, y = value)) +
    geom_point() +
    geom_line() +
    facet_wrap(~ statistic, ncol = 2, scales = "free")
}

# Plot the results
# Forward
regsub_plot(fwd_fit) +
  ggtitle("Forward Stepwise Selection")

# Backward
regsub_plot(bwd_fit) +
  ggtitle("Backward Stepwise Selection")
```

Variables selected by best subset selection are:
```{r, echo=FALSE}
# Select Variables
coef(best_fits, 5)
```
```{r, include=FALSE}
# Select Variables
coef(fwd_fit, 5)
coef(bwd_fit, 5)
```

We used best subset selection, forward stepwise selection, and backward stepwise selection methods. All three methods suggested using five predictors: `Pclass`, `Sex`, `Age`, `SibSp`, `Embarked`. However, we also tried training the models with all predictors. Results were more accurate for every model with all predictors than just using the five predicted suggested. Therefore, we opted to use all predictors instead.

# Model Selection
Before selecting any model, we recoded `Sex` into `female = 0`, `male = 1` and `Embarked` into `C = 0`, `Q = 1`, `S = 2`. This makes it easier to apply some of our models.


We used eight methods:

* Logistic regression
* Decision tree
* Bagging
* Random forest
* Boosting
* KNN
* LDA
* QDA

```{r, echo=FALSE, error=TRUE}
##Logistic regression
glm_fit <- glm(Survived ~., data = training, family = binomial)

glm_probs <- predict(glm_fit, training, type = "response")

training <- training %>%
    mutate(probs = glm_probs) %>%
    mutate(probs = ifelse(probs > 0.5, 1, 0))

###Testing accuracy
glm_probs <- predict(glm_fit, validation, type = "response")
validation <- validation %>%
    mutate(probs = glm_probs) %>%
    mutate(probs = ifelse(probs > 0.5, 1, 0))

attach(validation)
table(probs, Survived)
mean(probs == Survived)  #0.77

glm_preds <- predict(glm_fit, test, type = "response")
glm_test <- test %>% 
  mutate(Survived = ifelse(glm_preds > 0.5, 1, 0)) %>%
  dplyr::select(PassengerId, Survived)

write_csv(glm_test, "glm_test.csv")


##Classification trees
library(tree)
library(rattle)
training$Survived <- as.factor(training$Survived)
tree_fit <- tree(Survived ~., data = training)
summary(tree_fit)

tree_preds <- predict(tree_fit, validation, type = "class")
table(tree_preds, validation$Survived)
mean(tree_preds == validation$Survived) #0.79

set.seed(1)
cv_survived <- cv.tree(tree_fit, FUN = prune.misclass)
cv_survived

prune_survival <- prune.misclass(tree_fit, best = 7)
tree_preds <- predict(prune, validation, type="class")
table(tree_preds, validation$Survived)
mean(tree_preds == validation$Survived)

tree_preds <- predict(tree_fit, test, type = "class")
tree_test <- test %>% 
  mutate(Survived = tree_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(tree_test, "tree_test.csv")


##Bagging 
library(randomForest)
set.seed(1)
bag_fit <- randomForest(Survived ~., data = training, mtry = 7, importance = TRUE)
bag_fit

bag_preds <- predict(bag_fit, validation)
table(bag_preds, validation$Survived)
mean(bag_preds == validation$Survived)  #0.79

bag_preds <- predict(bag_fit, test)
bag_test <- test %>% 
  mutate(Survived = bag_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(bag_test, "bag_test.csv") #0.73502

##Random Forest
rf_fit <- randomForest(Survived ~., data = training, mtry = 2, importance = TRUE)
rf_preds <- predict(rf_fit, validation)
table(rf_preds, validation$Survived)
mean(rf_preds == validation$Survived)  #0.80

rf_preds <- predict(rf_fit, test)
rf_test <- test %>% 
  mutate(Survived = rf_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(rf_test, "rf_test.csv")  #0.78947


##Boosting
library(gbm)
set.seed(1)
boost_fit <- gbm(Survived ~., data = training, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
boost_preds <- predict(boost_fit, validation, n.trees = 5000)
boost_preds <-  ifelse(boost_preds > 1.5, 1, 0)
table(boost_preds, validation$Survived)
mean(boost_preds == validation$Survived)  #0.79

boost_preds <- predict(boost_fit, test, n.trees = 5000)
boost_preds <-  ifelse(boost_preds > 1.5, 1, 0)
boost_test <- test %>% 
  mutate(Survived = boost_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(boost_test, "boost_test.csv")  #0.77511

##KNN
library(class)
train_X <- training %>% dplyr::select(-Survived)
test_X <- validation %>% dplyr::select(-Survived)
train_survival <- training %>% dplyr::select(Survived) 

set.seed(1)
knn_preds <- knn(train_X, test_X, t(train_survival), k = 3)
table(knn_pred, validation_knn$Survived)
mean(knn_pred == validation_knn$Survived)  #0.67

test_X <- test %>% dplyr::select(Pclass, Sex, Age, SibSp, Parch, Fare, Embarked)
knn_preds <- knn(train_X, test_X, t(train_survival), k = 3)

knn_test <- test %>% 
  mutate(Survived = knn_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(knn_test, "knn_test.csv") #0.57894

##LDA 
library(MASS)
lda_fit <- lda(Survived ~., data = training)
lda_fit
plot(lda_fit)

lda_predict <- predict(lda_fit, validation)
names(lda_predict)
lda_class <- lda_predict$class
table(lda_class, validation$Survived)
mean(lda_class == validation$Survived) #0.76

lda_preds <- predict(lda_fit, test)$class
lda_test <- test %>% 
  mutate(Survived = lda_preds) %>%
  dplyr::select(PassengerId, Survived)

write_csv(lda_test, "lda_test.csv")  #0.75119

##QDA
qda_fit <- qda(Survived ~., data = training)
qda_fit
plot(qda_fit)

qda_predict <- predict(qda_fit, validation)
names(qda_predict)
qda_class <- qda_predict$class
table(qda_class, validation$Survived)
mean(qda_class == validation$Survived) #0.77

qda_preds <- predict(qda_fit, test)$class
qda_test <- test %>% 
  mutate(Survived = qda_preds) %>%
  dplyr::select(PassengerId, Survived)
```

Their respective accuracies are listed below:
```{r, echo=FALSE}
Model <- c("Logistic Regression", "KNN", "LDA", "QDA", "Decision Tree", "Bagging",  "Random Forest", "Boosting")
Accuracy <- c(0.74612, 0.57894, 0.75119, 0.76076, 0.79904, 0.73502, 0.78947, 0.77511)
tibble(Model, Accuracy) %>% arrange(desc(Accuracy))
```

All models work fairly well except for KNN. Decision tree provides the best accuracy at 0.799.

We also uploaded our analysis to Kaggle and ranked 1580th among over 10,000 competitors.
![Our ranking on the competition](kaggle_ranking.png)

# Discussion

# Discussion
Most of our models worked fairly well. Except for KNN, all models get an accuracy of above 0.7. Our best model, the Decision Tree, got an accuracy of 0.799, which means that we are able to correctly predict the survival of a passenger nearly 80% of the time, given their cabin class, sex, age, # of siblings or spouse and # of parents or children on board, port of embarkment, and ticket fare.

Although our models fared quite well, the leaders of the Kaggle competition achieved an accuracy of 1. Moving further, we can try combining the models and giving them different weights to improve accuracy. We can also try finding additional data, such as the distance of each passenger's cabin to deck.








